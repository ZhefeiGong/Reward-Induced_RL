
### Implementation Task - Reward-Induced

##### 0.TASKs

* [x] Implement the reward-induced representation learning model (see detailed architecture Figure 5). Train the model using the provided dataset. 

* [x] Visualize your training results (learning curves etc) and verify that the model predicts correctly by replicating the results in Figure 2. Note, that you need to add **a detached decoder network** and **train the model on a single reward** only to reproduce the experiment. 

* [x] Implement an RL algorithm of your choice to train **an agent** that can **follow a target shape** (while ignoring distractor shapes) in the provided environment. Before continuing to the next step, verify your implementation by **training a policy** that has access to **the true state of the environment** (i.e. does not need to encode images). This corresponds to **the oracle baseline** in Figure 3. 

* [x] Train **an image-based agent** that **encodes image observations** using **the pre-trained encoder**. Compare **its learning curve** to that of an agent with the same architecture, but trained from scratch (image-scratch baseline in Figure 3). 

* [x] If your implementation is working, (1) **the image-based agent** with pre-training should be able to follow the target shape with **up to one distractor**, and (2) it should learn faster than the **image-based agent** trained from scratch (but likely slower than the oracle).

<img src="src/Implementation-Task.png" width="700">
In complex environments we need to learn **representations** that focus on modeling **the important aspects of the environment**
We propose to use **task information** to guide representations as to which parts of the environment are important to model. 

* Left: 
	* We pre-train **a representation** by **inferring reward values for multiple tasks from a shared representation of the high-dimensional input**, 
	* encouraging it to focus on parts of the environment **that are important for solving the training tasks**,
	* e.g. it will focus on the **arm** and the **furniture** pieces instead of **the background texture**. 
* Right: 
	* We can **transfer** this representation into a policy for more efficient learning of downstream tasks 
	* from **the same task distribution** that the representation training tasks were drawn from,
	* e.g. tasks focused on robot and furniture pieces like lifting the chair to a certain height.

##### 1.Introduction
ğŸŒ…
* People have **the remarkable ability** to **condense** this raw input data stream **into a much compressed representation** on which **the decision making system operates**.
* We need learn to extract useful information from the input using a typically sparse reward signal.
* $\Rightarrow$ To improve **training efficiency** it has become common practice to **pre-train feature extractors** that **reduce the input dimensionality** while trying to retain as much information as possible
ğŸŒ…
* There are two dominant paradigms for the unsupervised pre-training of such feature extractors: 
	* **Generative**
		The generative approach directly trains a model of the high-dimensional input data via **reconstruction** or **prediction** objectives, constraining an intermediate representation within the model to be low-dimensional, e.g. in VAEs or predictive models.
	* **Discriminative Modeling**
		The discriminative approach instead optimizes a lower-bound on **mutual information** between the high-dimensional input and the representation using noise-contrastive estimation.
* Such approaches are forced to 
	* model all information in the input data
	* cannot **discriminate** between what is useful to model and which **input information** can be ignored
* Highly expressive models are required for modeling all facets of the environment and as a result the efficiency gains for downstream learning reported in prior work only considered relatively clean environments of low detail and complexity
* ğŸ‰In order to scale RL from high-dimensional observations to more complex environments we need representation learning methods that can focus on the important information in the input data
ğŸŒ…
* **Propose** to use task information for guiding representation learning approaches as to which part of the input information is important to model and which parts can be ignored
* Propose to use **a large dataset of trajectories annotated with rewards** from a multitude of tasks **drawn from the task distribution of interest**
* We train **predictive models** over these **reward labels** for learning **a reward-induced representation** that captures only information that 
	* **useful for predicting rewards**
	* **useful for solving tasks from this task distribution**

##### 2.Related Work
* There has been a wide range of works that apply different forms of unsupervised representation learning to improve the sample efficiency of reinforcement learning, both generative/predictive models as well as discriminative models
* ğŸ‰**None** of these methods can **discriminate** **between useful and distracting information** in the input and therefore have only been shown to **work on environments** that **do not match** the complexity of **the real world**
* Our work specifically focuses on **learning a representation** that can be used for **solving arbitrary downstream tasks** drawn from **the same task distribution that was used for training the representation**

##### 3.Approach

ğŸŒPreliminaries
* Assume access to a pre-training dataset D of reward-annotated trajectories $Ï„ = \{s^t, a^t, r^t_{1:K} , s^{t+1}, a^{t+1}, r^{t+1}_{1:K} , . . . \}$, with states $s_t$, actions $a_t$ and rewards $r^t_{1:K}$ for $K$ tasks drawn from the task distribution $T$ .
* Note that we do not need to assume complete reward annotations from all $K$ tasks on all $|D|$ trajectories. Instead, a particular trajectory $Ï„_i$ can have reward annotations from only a subset $Tâ€² âŠ† T$ .
* For simplicity, we will **assume** **complete** reward annotation in the following, but the proposed model can be trivially extended to the **incomplete** case.
* Finally, we **do not make assumptions** on the exploration policy $Ï€_e$ that was used to collect the pre-training dataset 
	**other than** that the resulting trajectories need to provide meaningful interactions with the environment.

ğŸŒReward-Induced Representations
* A reward prediction model $p_Î¸(r_{1:K} | x)$ with parameters $Î¸$ to infer the rewards from $K$ different tasks given the input $x$.
	Factorize the prediction into 
	* An encoder $p_{\phi}(z|x)$
	* K reward-heads $p_{\eta_k}(r_k|z)$
	$$
	p(r_{1:K}|x) = \prod^K_{k=1}âˆ« p_{\eta_k} (r_k|z) Â· p_{\phi}(z|x) dz
	$$
	Optimize the parameters $\theta = \{\phi, \eta_1 : K\}$ using a maximum-likelihood objective on the predicted rewards
	
	we can optimize the $MSE$ loss on the predicted rewards $\hat{r}_k$ to learn the representation. For a single training trajectory from the pre-training dataset this takes the form : 
	$$
	L= \sum^T_{t=1}\sum^{K}_{k=1}||r^t_kâˆ’\hat{r}^t_k||^2
	$$
* The single-step inference case $p(r{1:K}|x)$. It is however easily extendible to the sequence prediction case, where we predict $T$ future rewards given $N$ conditioning frames
	$$
	p(r^{1:T}_{1:K}|x^{âˆ’N+1:0}) = \prod^T_{t=1}\prod^K_{k=1}âˆ«p_{\eta_k}(r^t_k|z^t)Â·p_{\phi}(z^t|x_{âˆ’N+1:tâˆ’1})dz
	$$
	In practice we implement this recursive prediction with **an RNN-based** model and optimize with the same objective

ğŸŒReinforcement Learning with Pre-Trained Representation
* Train policies to optimize the cumulative expected return on downstream tasks: $arg max_Ï€ E_Ï€ [ âˆ‘^T_{t=1} R_t ]$. 
* To use the pre-trained representation we factorize the policy distribution, using the pre-trained encoder to translate inputs into the representation $z$ : 
	$$
	Ï€(a|x) = Ï€â€²(a|z) Â· p_{\phi}(z|x)
	$$
* The optimization of this objective can be performed with any standard RL algorithm, for example value-based approaches or policy gradient methods.


##### 4.Experimental Evaluation

ğŸŒ•We aim to answer the following questions : 
1. Are **reward-induced representations** 
	**helpful for improving the sample efficiency** of downstream tasks?
2. Are **reward-induced representations** 
	more **robust to visual distractors** in the scene?

ğŸŒ•Instantiate 
* the **encoder** and **decoder** with simple CNNs
* **the predictive model** with a single-layer LSTM and all reward-heads with 3-layer MLPs. 
* **the Rectified Adam optimizer** with $Î²_1 = 0.9$ and $Î²_2 = 0.999$ for pre-training the representation. 
* For RL training, we use $PPO$ with slight modifications to the default hyper-parameters provided in the PyTorch implementation

ğŸŒ•Environment
The environment features 
* an agent (circle shape)
* a target (square)
* a variable number of distractor objects (triangles)

**For pre-training** : 
	we collect random rollouts annotated with rewards proportional to the x/y position of both agent and target.
**During RL training** : 
	âœ¨During RL training we evaluate on a downstream task in which the agent should **follow** the target.
	âœ¨Eg : 
		Minimize its L2 distance to the target. 
		We use the following reward (where $p_{target}$ and $p_{agent}$ denote targetâ€™s and agentâ€™s position)
		$$
		R=1âˆ’ \frac{1}{\sqrt{2}} Â· â€–p_{target} âˆ’ p_{agent}â€–^2
		$$
ğŸŒ•Baselines
For **downstream RL experiments** we compare to the following baselines : 
* âœ¨cnn
	* Baseline uses a 3-layer CNN with 16 kernels of kernel size 3 and stride 2 to encode an image. 
	* ReLU is used as an non-linear activation. 
	* The output activation is flattened and passed to two fully connected layers of 64 hidden states 
		* to compute 
			* an action 
			* a critic
* âœ¨image-scratch
	* Baseline adopts the encoder architecture used in representation learning but we **initialize the parameters randomly**. 
	* The output embedding of size 64 is fed into **two 2-layer MLPs** of **32 hidden states**, **one for action** and **one for critic**
* âœ¨image-reconstruction and image-reconstruction finetune
	 * Baselines use the learned representation on the **image reconstruction** task. 
	 * Given the embedding the action distribution and critic are computed using 2-layer MLPs of 32 hidden states. 
	 * We 
		 * **freeze** the encoder for the baseline (image-reconstruction) 
		 * **finetune** the encoder for the baseline (image-reconstruction finetune).
* âœ¨reward-prediction and reward-prediction finetune
	* Baselines use the learned representation on **the reward prediction** task.
	* Given the embedding the action distribution and critic are computed using 2-layer MLPs of 32 hidden states. 
		* We 
			* freeze the encoder for the baseline (reward-prediction) 
			* finetune the encoder for the baseline (reward-prediction finetune).
* âœ¨oracle
	* Take **a state representation** as input. 
	* The state representation consists of $(x,y)$ coordinates of the agent, target, and distractors. 
	* The 2-layer MLPs of 32 hidden states are used to predict both action and critic. 
	* This method shows **the upper bound** of our method.

ğŸŒ•Analysis of Learned Representation

âœ¨
* First
	Analyze the properties of reward-induced representations.
	We try to elicit 
	* What information is captured in the representation by training an image decoder to **reconstruct** the original image from the representation.
	* The gradients from the image decoder are stopped before the representation, leaving the ladder unchanged by the probing networkâ€™s training.

âœ¨
* For two simple rewards indicate that the reward-induced representation indeed **only captures information** about the input that **are useful for inferring reward**
	* As the decoder is not able to infer the position of the object along the axis that has no influence on the reward used for training the representation.	
	  ğŸ“Š
	
	  <img src="src/reward-induce_2.png" width="700">Detached decodings of reward-induced representations. 
	
	* Top: Ground truth sequence. 
	
	* Middle: Decoding of representation learned with reward proportional to the vertical coordinate. 
	
	* Bottom: Same, but with reward proportional to horizontal coordinate. 
	Representations retain **information about the position that influences the reward**, but **no information about the other coordinate**, which **leads to blurry predictions on the latter axis**.

âœ¨
* To also quantify how **much** of **the important information** is captured **in reward-induced representations** compared to **conventional**, **image-based representations**. 
	* We compare **values for different numbers of visual distractors** in the scene and find that **reward-induced representations** are **better able to capture the important information** in the scene across all scenarios.
	
	* Further, they prove **fairly robust to increased noise**
	
	* Representation learned via image prediction objectives on the other hand 
		are **not able to capture all important information**, leading to **worse regression accuracy**, 
		because **during training** they **receive no guidance** on what is important to model and what can be ignored.
		ğŸ“Š
	
		<img src="src/reward-induce_t1.png" width="700">
		Reward regression MSE values for different representations with no, one and two distractors.
		
	* A-X denotes reward proportional to the **agentâ€™s horizontal position**
	
	* T-Y indicates reward proportional to the **targetâ€™s vertical position**
	  The reward-induced representation 
	
	* enables more accurate reward regression 
	
	* is also fairly robust to increasing amounts of noise 
	
	* while the image prediction based representation fails to capture information necessary to regress all rewards accurately.



ğŸŒ•Reward-Induced Representations for Reinforcement Learning

âœ¨

<img src="src/reward-induce_3.png" width="700">
We compare the speed of training and convergence across all the baselines discussed above. 

* Oracle baseline acts over the most compact state representation containing shape positions and consequently performs the best. 
* reward-prediction finetune comes second both in speed and final convergence for environments with zero and one distractor. 
* cnn learns slower than our method, but converges to similar values in the end. 
* Other methods struggle to attain similar performance. 
This shows that **reward prediction** leads to **a good encoder initialization for downstream tasks**, and this **is faster than a randomly initialized CNN encoder**. 
It is **much better than image_scratch** which uses a similarly sized encoder as our method. 
Note that cnn performs better than image_scratch because of **its small sized CNN network** suitable for RL.

âœ¨

<img src="src/reward-induce_4.png" width="700">

* It shows qualitative rollouts of reward-prediction fine-tune upon convergence on the follow task with **0** and **1** distractor.
* The agent (circle) successfully follows the target (square) even in the presence of distractors (triangles).

Similar performance cannot be attained with 2 distractors.
* This can be attributed to the reward prediction possibly overfitting during training. Therefore, the oracle method can be seen to perform very well, followed by the standard cnn policy.
* Interestingly, image_reconstruction finetune also performs well on this task since it is able to maintain information about the agent, target and distractors, which can be extracted by the RL policy.
* However, this is still slower in comparison to the small CNN network based policy.

While we can show that our method is more robust than baselines in the case of a single distractor, we hope to further improve our method on multiple distractors with better training schemes.

##### 5.Discussion
* Reward-induced representations are able to improve the learning efficiency of downstream reinforcement learning applications more efficiently than conventional, image-predictionbased representations.
* we showed that reward-guidance improves the robustness to visual distractors in the scene, an important step towards the applicability of representation learning methods to the real world.


##### 6.Architecture - the reward prediction model
<img src="src/reward-induce_5.png" width="700">

* All MLPs have **3 layers with 32 hidden units**. 
* The image encoder uses **strided convolutions** to reduce the image resolution by a factor of 2 in every layer, until the spatial resolution is 1x1 (i.e. the number of layers is determined by the input resolution). 
* The number of channels gets doubled in every layer starting with 4 channels in the first layer. 
* The final 1x1 feature vector gets mapped to **a 64-dimensional observation space** using a linear layer. 
* The **encoder** (**green**) is transferred to the RL policy, where it is used to encode the image inputs.
* LSTM
	* ğŸŒ	<img src="src/LSTM.png" width="700">
	* ğŸŒ		<img src="src/lstm_frame.png" width="700">

##### 7.Actor-Critic
1. DQN - CRITIC(like)
	* TD error(Temporal Difference error)
		* $TDerror=r+\gamma V(s')-V(s)$
			* The definition of TD error is based on state transitions in a Markov Decision Process (MDP). Given a state $s$, when an agent takes an action $a$ and observes the next state $sâ€²$ along with an immediate reward $r$, the TD error can be expressed as the above equation.
		* $TD error = r+\gamma max_{a'}Q(s',a')-Q(s,a)$
			* The TD error in DQN
	* The **Critic**'s task is to make the TD-error as small as possible. Then TD-error updates the Actor.
		* In order to avoid the positive number trap, we want the Actor's update weight to be positive or negative. Therefore, we subtract their mean $V$ from the $Q$ value. have, $Q(s,a)-V(s)$
		* In order to avoid the need to estimate $V$ and $Q$ values, we hope to unify $Q$ and $V$, Due to $Q(s,a) = \gamma * V(s') + r - V(s)$, so we get TD-errorï¼š $TD-error = \gamma * V(s') + r - V(s)$
		* TD-error is the weight value in the weighted update when the Actor updates the strategy.
		* Now the critic no longer needs to estimate $Q$, but estimates $V$. According to what we have learned from the Markov chain, we know that TD-error is the loss required by the Critic network. In other words, the Critic function needs to minimize TD-error.

2. Policy Gradient - ACTOR(like)
	* The **reward** function is defined as : 
		$J(\theta)=\sum_{s\in S} d^{\pi}(s)V^{\pi}(s)=\sum_{s\in S}d^{\pi}(s)\sum_{a\in A}\pi_{\theta}(a|s)Q^{\pi}(s,a)$
		* whereÂ $d^{\pi}(s)$ is the stationary distribution of Markov chain forÂ $\pi_{\theta}$Â (on-policy state distribution underÂ $\pi$).
	* REINFORCE(Monte-Carlo)
		* **REINFORCE**Â (**Monte-Carlo** policy gradient) relies on an estimated return by Monte-CarloÂ methods using episode samples to update the policy parameterÂ $\theta$. REINFORCE works because the expectation of the sample gradient is equal to the actual gradient :
			* $\nabla_{\theta}J(\theta) = E_{\pi}[Q^{\pi}(s,a)\nabla_{\theta}ln\pi_{\theta}(a|s)] = E_{\pi}[G_t\nabla_{\theta}ln\pi_{\theta}(A_t|S_t)]$

3. **Actor-Critic**
	* Two main components in policy gradient are **the policy model** and **the value function**. It makes a lot of sense to learn the value function in addition to the policy, since knowing the value function can assist the policy update, such as by reducing gradient variance in vanilla policy gradients, and that is exactly what theÂ **Actor-Critic**Â method does.
	* Actor-critic methods consist of two models, which may optionally share parameters:
		* Critic updates the value function parameters w and depending on the algorithm it could be **action-value**Â $Q_w(a|s)$Â or **state-value** $V_w(s)$
		* Actor updates the policy parameters $\theta$ for $\pi_{\theta}(a|s)$, in the direction suggested by the critic.
	* A simple action-value actor-critic algorithm - **QAC** - from [Lil'Log](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)	
		1. Initialize $s,\theta,w$ at random; sample $a$ ~ $\pi_{\theta}(a|s)$
		2. For $t$ = $1...T$ : 
			1. Sample rewardÂ $r_t$ ~ $R(s,a)$Â and next state $s'$ ~ $P(s'|s,a)$
			2. Then sample the next actionÂ $a'$~$\pi_{\theta}(a'|s')$
			3. Update the policy parameters: $\theta \Leftarrow \theta+ \alpha_{\theta}Q_{w}(s,a)\nabla_{\theta}ln\pi_{\theta}(a|s)$
			4. Compute the correction (TD error) for action-value at time t:
				$\delta_t=r_t + \gamma Q_w(s',a')-Q_w(s,a)$
				and use it to update the parameters of action-value function : 
				$w\leftarrow w+\alpha_w \delta_t\nabla_wQ_w(s,a)$
			5. Update $a\leftarrow a'$ and $s\leftarrow s'$
		* Two learning rates,Â $\alpha_{\theta}$Â and$\alpha_w$, are predefined for policy and value function parameter updates respectively.
		* QAC		<img src="src/QAC.png" width="700">
		
	* Advantage Actor Critic - **A2C** - from [Chris Yoon](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)
		* There're two function approximations
			* Actor, a policy function parameterized by $\theta$ : $\pi_{\theta}(s,a)$
			* Critic, a value function parameterized by $w$ : $\hat{q}_w(s,a)$
		* Equations
			* Advantage Value
				$A(s_t,a_t)=Q_w(s_t,a_t)-V_v(s_t)$
				* $Q(s_t,a_t)=E[r_{t+1}+\gamma V(s_{t+1})]$
				* $A(s_t,a_t)=r_{t+1}+\gamma V_v(s_{t+1})-V_v(s_t)$
			* The update equation : 
				$\nabla_{\theta}J(\theta)$ ~ $\sum_{t=0}^{T-1}\nabla_{\theta}log\pi_{\theta}(a_t|s_t)(r_{t+1}+\gamma V_v(s_{t+1})-V_v(s_t))$
				$\nabla_{\theta}J(\theta)$ = $\sum_{t=0}^{T-1}\nabla_{\theta}log\pi_{\theta}(a_t|s_t)A(s_t,a_t)$
		
	* Asynchronous Advantage Actor-Critic (A3C) - from [Lil'Log](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#dpg)
		* In A3C, the critics learn the value function while multiple actors are **trained in parallel** and get synced with global parameters from time to time. Hence, A3C is designed to work well for parallel training.
		* Letâ€™s use the state-value function as an example. 
			* The loss function for state value is to minimize the mean squared error,Â $J_{v}(w)=(G_t-V_w(s))^2$
			* Gradient descent can be applied to find the optimal w. This state-value function is used as the baseline in the policy gradient update.
		* Here's the algorithm outline
			1. We have global parameters,Â $\theta$ and $w$; similar thread-specific parameters,Â $\theta'$ and $w'$.
			2.  Initialize the time step $t=1$
			3. While $T \leq T_{MAX}$
				1. Reset gradient: $d\theta=0$Â andÂ $dw=0$
				2. Synchronize thread-specific parameters with global ones : $\theta'=\theta$ and $w'=w$
				3. $t_{start}=t$ and sample a starting state $s_t$
				4. While ($s_t != TERMINAL$) and $t-t_{start}\leq t_{max}$ : 
					1. Pick the action $A_{t}$ ~ $\pi_{\theta'}(A_t|S_t)$ and receive a new reward $R_t$ and a new state $s_{t+1}$
					2. Update $t+t+1$ and $T=T+1$
				5. Initialize the variable that holds the return estimation
					$$
					R=
					\begin{cases}
					0 & if\ s_t\ is\ TERMINAL\\
					V_{w'}(s_t) & otherwise
					\end{cases}
					$$
				6. For $i=t-1,...,t\_start$
					1. $R\leftarrow \gamma R+R_i$; here $R$ is a MC measure of $G_i$
					2. Accumulate
						1. Accumulate gradients w.r.t. 
							$\theta' : d\theta \leftarrow d\theta + \nabla_{\theta'}log\pi_{\theta'}(a_i|s_i)(R-V_{w'}(s_i))$
						2. Accumulate gradients w.r.t.
							$w'$ : $dw \leftarrow dw + 2(R-V_{w'}(s_i))\nabla_{w'}(R-V_{w'}(s_i))$
				7. Update asynchronously $\theta$ using $d\theta$, and $w$ using $dw$
				A3C enables the parallelism in multiple agent training. The gradient accumulation step (6.2) can be considered as a parallelized reformation of minibatch-based stochastic gradient update: the values ofÂ $w$Â orÂ $\theta$Â get corrected by a little bit in the direction of each training thread independently.

##### 8.PPO

ğŸŒFrom [PPO paper](https://arxiv.org/abs/1707.06347)
* Background: Policy Optimization
	* Policy Gradient Methods
	* Trust Region Methods
* Clipped Surrogate Objective 
	
* Adaptive KL Penalty Coefficient
	
* Algorithm : <img src="src/PPO_paper_code.png" width="700">


ğŸŒFrom [zhihu](https://zhuanlan.zhihu.com/p/468828804) <<-->> **PPO-Penalty**
* Equation
	* $\overline{R}=\sum^{T}_{t=1}\frac{p_{\theta}(a_t|s_t)}{p_{\theta'}(a_t|s_t)} A_t(s_t,a_t)-\lambda KL[\theta,\theta']$
	* $A^{\theta}(s_t,a_t)=\sum_{t'>t}\gamma^{t'-t}r_{t'}-V_{\phi}(s_t)$
* Key Words
	* **Importance Sampling**
		$\theta' \rightarrow \theta$
		* Sample the data from $\theta'$
		* Use the data to train $\theta$ many times 
		Use KL Divergence

>1ï¸âƒ£0ç‚¹æ—¶ï¼šæˆ‘ä¸ç¯å¢ƒè¿›è¡Œäº’åŠ¨ï¼Œæ”¶é›†äº†å¾ˆå¤šæ•°æ®ã€‚ç„¶ååˆ©ç”¨æ•°æ®æ›´æ–°æˆ‘çš„ç­–ç•¥ï¼Œæ­¤æ—¶æˆ‘æˆä¸º1ç‚¹çš„æˆ‘ã€‚å½“æˆ‘è¢«æ›´æ–°åï¼Œç†è®ºä¸Šï¼Œ1ç‚¹çš„æˆ‘å†æ¬¡ä¸ç¯å¢ƒäº’åŠ¨ï¼Œæ”¶é›†æ•°æ®ï¼Œç„¶åæŠŠæˆ‘æ›´æ–°åˆ°2ç‚¹ï¼Œç„¶åè¿™æ ·å¾€å¤è¿­ä»£ã€‚
>2ï¸âƒ£ä½†æ˜¯å¦‚æœæˆ‘ä»ç„¶æƒ³ç»§ç»­0ç‚¹çš„æˆ‘æ”¶é›†çš„æ•°æ®æ¥è¿›è¡Œæ›´æ–°ã€‚å› ä¸ºè¿™äº›æ•°æ®æ˜¯0ç‚¹çš„æˆ‘ï¼ˆè€Œä¸æ˜¯1ç‚¹çš„æˆ‘ï¼‰æ‰€æ”¶é›†çš„ã€‚æ‰€ä»¥ï¼Œæˆ‘è¦å¯¹è¿™äº›æ•°æ®åšä¸€äº›é‡è¦æ€§é‡é‡‡æ ·ï¼Œè®©è¿™äº›æ•°æ®çœ‹èµ·æ¥åƒæ˜¯1ç‚¹çš„æˆ‘æ‰€æ”¶é›†çš„ã€‚å½“ç„¶è¿™é‡Œä»…ä»…æ˜¯çœ‹èµ·æ¥åƒè€Œå·²ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦å¯¹è¿™ä¸ªâ€œä¸åƒâ€çš„ç¨‹åº¦åŠ ä»¥æ›´æ–°æ—¶çš„æƒ©ç½šï¼ˆKLï¼‰ã€‚
>3ï¸âƒ£å…¶ä¸­ï¼Œæ›´æ–°çš„æ–¹å¼æ˜¯ï¼šæˆ‘æ”¶é›†åˆ°çš„æ¯ä¸ªæ•°æ®åºåˆ—ï¼Œå¯¹åºåˆ—ä¸­æ¯ä¸ªï¼ˆs, aï¼‰çš„ä¼˜åŠ¿ç¨‹åº¦åšè¯„ä¼°ï¼Œè¯„ä¼°è¶Šå¥½çš„åŠ¨ä½œï¼Œå°†æ¥å°±åˆåœ¨sçŠ¶æ€æ—¶ï¼Œè®©aå‡ºç°çš„æ¦‚ç‡åŠ å¤§ã€‚è¿™é‡Œè¯„ä¼°ä¼˜åŠ¿ç¨‹åº¦çš„æ–¹æ³•ï¼Œå¯ä»¥ç”¨æ•°æ®åé¢çš„æ€»æŠ˜æ‰£å¥–åŠ±æ¥è¡¨ç¤ºã€‚å¦å¤–ï¼Œè€ƒè™‘å¼•å…¥åŸºçº¿çš„Tipï¼Œæˆ‘ä»¬å°±åˆå¼•å…¥ä¸€ä¸ªè¯„ä»·è€…å°æ˜ï¼Œè®©ä»–è·Ÿæˆ‘ä»¬ä¸€èµ·å­¦ä¹ ï¼Œä»–åªå­¦ä¹ æ¯ä¸ªçŠ¶æ€çš„æœŸæœ›æŠ˜æ‰£å¥–åŠ±çš„å¹³å‡æœŸæœ›ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬è¯„ä¼°ï¼ˆs, aï¼‰æ—¶ï¼Œæˆ‘ä»¬å°±å¯ä»¥å§å°æ˜å¯¹ s çš„è¯„ä¼°ç»“æœå°±æ˜¯ s çŠ¶æ€åç»­èƒ½è·å¾—çš„æŠ˜æ‰£æœŸæœ›ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„åŸºçº¿ã€‚æ³¨æ„å“ˆï¼šä¼˜åŠ¿å‡½æ•°ä¸­ï¼Œå‰ä¸€åŠæ˜¯å®é™…æ•°æ®ä¸­çš„æŠ˜æ‰£æœŸæœ›ï¼Œåä¸€åŠæ˜¯ä¼°è®¡çš„æŠ˜æ‰£æœŸæœ›ï¼ˆå°æ˜å¿ƒä¸­è®¤ä¸ºsåº”è¯¥å¾—åˆ°çš„åˆ†æ•°ï¼Œå³å°æ˜å¯¹sçš„æœŸæœ›å¥–åŠ±ï¼‰ï¼Œå¦‚æœä½ é€‰å–çš„åŠ¨ä½œå¾—åˆ°çš„å®é™…å¥–åŠ±æ¯”è¿™ä¸ªå°æ˜å¿ƒä¸­çš„å¥–åŠ±é«˜ï¼Œé‚£å°æ˜ä¸ºä½ æ‰“æ­£åˆ†ï¼Œè®¤ä¸ºå¯ä»¥æé«˜è¿™ä¸ªåŠ¨ä½œçš„å‡ºç°æ¦‚ç‡ï¼›å¦‚æœé€‰å–çš„åŠ¨ä½œçš„å®é™…å¾—åˆ°çš„å¥–åŠ±æ¯”å°æ˜å¿ƒä¸­çš„æœŸæœ›è¿˜ä½ï¼Œé‚£å°æ˜ä¸ºè¿™ä¸ªåŠ¨ä½œæ‰“è´Ÿåˆ†ï¼Œä½ åº”è¯¥å‡å°è¿™ä¸ªåŠ¨ä½œçš„å‡ºç°æ¦‚ç‡ã€‚è¿™æ ·ï¼Œå°æ˜å°±æˆä¸ºäº†ä¸€ä¸ªè¯„åˆ¤å®˜ã€‚
>4ï¸âƒ£å½“ç„¶ï¼Œä½œä¸ºè¯„åˆ¤å®˜ï¼Œå°æ˜è‡ªèº«ä¹Ÿè¦æé«˜è‡ªå·±çš„çŸ¥è¯†æ–‡åŒ–æ°´å¹³ï¼Œä¹Ÿè¦åœ¨æ•°æ®ä¸­ä¸æ–­çš„å­¦ä¹ æ‰“åˆ†æŠ€å·§ï¼Œè¿™å°±æ˜¯å¯¹Â $\phi$ çš„æ›´æ–°äº†ã€‚


ğŸŒFrom [Lil'Log](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#dpg) <<-->> Update from TRPO <<-->> **PPO-Clip**
* Without a limitation on the distance betweenÂ $\theta_{old}$Â andÂ $\theta$, to maximizeÂ $J^{TRPO}(\theta)$Â would lead to instability with extremely large parameter updates and big policy ratios. PPO imposes the constraint by forcingÂ $r(\theta)$Â to stay within a small interval around $1$, preciselyÂ $[1âˆ’\epsilon,1+\epsilon]$, whereÂ $\epsilon$Â is a hyper-parameter. 
	* $J^{CLIP}(\theta)=E[min(r(\theta)\hat{A}_{\theta_{old}}(s,a), clip(r(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_{\theta_{old}}(s,a))]$
* When applying PPO on the network architecture with shared parameters for both policy (actor) and value (critic) functions, in addition to **the clipped reward**, the objective function is augmented with **an error term on the value estimation** and **an entropy term** to encourage sufficient exploration.
	* $J^{CLIP'}(\theta)=E[J^{CLIP}(\theta)-c_1(V_{\theta}(s)-V_{target})^2 + c_2H(s,\pi_{\theta}(.))]$


ğŸŒFrom [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
* ğŸŒ•TRPO
	* BG
		* TRPO updates policies by taking the largest step possible to improve performance, while satisfying a special constraint on how close the new and old policies are allowed to be. The constraint is expressed in terms ofÂ [KL-Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), a measure of (something like, but not exactly) distance between probability distributions.
		
	* Facts
		* on-policy algorithm
		* useful for environments with either discrete or continuous action spaces
		
	* Equations
		<img src="src/TRPO_1.png" width="700">
		
		<img src="src/TRPO_2.png" width="700">
		
		<img src="src/TRPO_3.png" width="700">
		
		* But $H^{-1}$ isÂ the second-order derivative and its inverse, a very expensive operation.
		
	* Pseudocode
		<img src="src/TRPO_code.png" width="700">
	
* ğŸŒ•PPO
	* BG
		PPO is motivated by the same question as TRPO: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse? Where TRPO tries to solve this problem with a complex second-order method, PPO is a family of first-order methods that use a few other tricks to keep new policies close to old. PPO methods are significantly simpler to implement, and empirically seem to perform at least as well as TRPO.
		There are two primary variants of PPO: PPO-Penalty and PPO-Clip.
		* **PPO-Penalty**Â approximately solves a KL-constrained update like TRPO, but penalizes the KL-divergence in the objective function instead of making it a hard constraint, and automatically adjusts the penalty coefficient over the course of training so that itâ€™s scaled appropriately.
		* **PPO-Clip**Â doesnâ€™t have a KL-divergence term in the objective and doesnâ€™t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.
	* Facts
		- PPO is an on-policy algorithm.
		- PPO can be used for environments with either discrete or continuous action spaces.
	- Equations  - PPO-Clip
		- <img src="src/PPO-clip_1.png" width="700">
		- <img src="src/PPO-clip_2.png" width="700">
	- ğŸŒ…Pseudocode
		<img src="src/PPO-clip_code.png" width="700">


ğŸŒ From [Jonathan Hui](https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12)
* Motivation
	1. Q-learning (with function approximation) fails on many simple problems and is poorly understood
	2. Vanilla policy gradient methods have poor data efficiency and robustness
	3. Trust region policy optimization (TRPO) is relatively complicated, and is not compatible with architectures that include noise (such as dropout) or parameter sharing (between the policy and value function, or with auxiliary tasks).
* Abstract
	* PPO adds a soft constraint that can be optimized by a first-order optimizer. We may make some bad decisions once a while but it strikes a good balance on the speed of the optimization. 
	* Experimental results prove that this kind of balance achieves the best performance with the most simplicity.
* Method
	* PPO with KL Penalty
		<img src="src/PPO_kl_prov.png" width="700">
		![[PPO_kl.png" width="700">
	* PPO with Clipped Objective	
		
		<img src="src/PPO_cl_1.png" width="700">
		
		<img src="src/PPO_cl_2.png" width="700">
		
		<img src="src/PPO_cl_3.png" width="700">
		
		<img src="src/PPO_cl.png" width="700">


##### 9.TEMP
* Reward in this Paper: 
	$$
	R = 1 - \frac{1}{\sqrt{2}}||P_{target}-P_{agent}||_2
	$$
* To use the pre-trained representation we factorize the policy distribution, using the pre-trained encoder to translate inputs into the representation $z$:
	$$
	\pi(a|x)=\pi'(a|z)p_{\phi}(z|x)
	$$

* ğŸŒ<img src="src/PPO-clip_code.png" width="700">

ğŸŒ•Distribution
åœ¨PyTorchä¸­å®ç°Actor-Criticæ¨¡å‹æ—¶ï¼Œé€‰æ‹©åˆé€‚çš„åˆ†å¸ƒç±»å‹å¯¹äºæ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å½“å¤„ç†çš„æ˜¯è¿ç»­åŠ¨ä½œç©ºé—´ã€‚æ ¹æ®åŠ¨ä½œç©ºé—´çš„æ€§è´¨ï¼ˆè¿ç»­æˆ–ç¦»æ•£ï¼‰ï¼Œå¸¸ç”¨çš„åˆ†å¸ƒç±»å‹æœ‰ï¼š

* ğŸ”¥è¿ç»­åŠ¨ä½œç©ºé—´
	å¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œé€šå¸¸ä½¿ç”¨ä»¥ä¸‹ä¸¤ç§åˆ†å¸ƒï¼š
	1. **æ­£æ€åˆ†å¸ƒï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰**ï¼š
	   - å•å˜é‡æˆ–å¤šå…ƒæ­£æ€åˆ†å¸ƒï¼ˆ`torch.distributions.Normal` æˆ– `torch.distributions.MultivariateNormal`ï¼‰æ˜¯å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´æœ€å¸¸è§çš„é€‰æ‹©ã€‚å¯¹äºæ¯ä¸ªåŠ¨ä½œç»´åº¦ï¼Œæ¨¡å‹è¾“å‡ºåŠ¨ä½œçš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆæˆ–æ–¹å·®ï¼‰ï¼Œè¿™äº›å‚æ•°å®šä¹‰äº†åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒã€‚
	   - åœ¨å®è·µä¸­ï¼Œå¦‚æœåŠ¨ä½œç»´åº¦ç›¸äº’ç‹¬ç«‹ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ç»„ç‹¬ç«‹çš„æ­£æ€åˆ†å¸ƒï¼Œæ¯ä¸ªåŠ¨ä½œç»´åº¦ä¸€ä¸ªï¼›å¦‚æœåŠ¨ä½œç»´åº¦ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œåˆ™ä½¿ç”¨**å¤šå…ƒæ­£æ€åˆ†å¸ƒ**ï¼Œå®ƒé€šè¿‡åæ–¹å·®çŸ©é˜µæ•è·ç»´åº¦ä¹‹é—´çš„ç›¸å…³æ€§ã€‚
	2. **å¯¹è§’é«˜æ–¯åˆ†å¸ƒ**ï¼š
	   - å®é™…ä¸Šæ˜¯å¤šå…ƒæ­£æ€åˆ†å¸ƒçš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå…¶ä¸­åæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’çš„ã€‚è¿™æ„å‘³ç€ä¸åŒåŠ¨ä½œç»´åº¦çš„æ¦‚ç‡åˆ†å¸ƒè¢«å‡è®¾ä¸º**ç›¸äº’ç‹¬ç«‹**ï¼Œæ¯ä¸ªç»´åº¦ç”±å•ç‹¬çš„å‡å€¼å’Œæ–¹å·®å‚æ•°åŒ–ã€‚è¿™ç§åˆ†å¸ƒå¸¸å¸¸é€šè¿‡è‡ªå®šä¹‰å®ç°ï¼Œå¦‚ä¸Šæ–‡æåˆ°çš„`DiagGaussianDistribution`ï¼Œå®ƒåœ¨åº•å±‚ä½¿ç”¨`MultivariateNormal`ï¼Œä½†é™åˆ¶åæ–¹å·®çŸ©é˜µä¸ºå¯¹è§’çº¿å½¢å¼ã€‚
* ğŸ”¥ç¦»æ•£åŠ¨ä½œç©ºé—´
	å¯¹äºç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œé€šå¸¸ä½¿ç”¨ä»¥ä¸‹åˆ†å¸ƒï¼š
	1. **åˆ†ç±»åˆ†å¸ƒï¼ˆCategorical Distributionï¼‰**ï¼š
	   - `torch.distributions.Categorical`ç”¨äºæ¨¡å‹è¾“å‡ºä¸ºç¦»æ•£åŠ¨ä½œæ¦‚ç‡çš„æƒ…å†µã€‚è¿™é€‚ç”¨äºæœ‰é™ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œæ¨¡å‹ä¼šè¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼Œç„¶åæ ¹æ®è¿™äº›æ¦‚ç‡é‡‡æ ·åŠ¨ä½œã€‚è¿™æ˜¯å®ç°ç¦»æ•£åŠ¨ä½œç©ºé—´ç­–ç•¥æœ€ç›´æ¥çš„æ–¹æ³•ã€‚
	2. **ä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆBernoulli Distributionï¼‰**ï¼š
	   - å¯¹äºäºŒå…ƒåŠ¨ä½œç©ºé—´ï¼ˆå³æ¯ä¸ªåŠ¨ä½œåªæœ‰ä¸¤ç§å¯èƒ½çš„ç»“æœï¼‰ï¼Œ`torch.distributions.Bernoulli`å¯èƒ½æ˜¯æ›´åˆé€‚çš„é€‰æ‹©ã€‚å®ƒé€šå¸¸ç”¨äºæ¯ä¸ªåŠ¨ä½œå¯ä»¥çœ‹ä½œç‹¬ç«‹çš„æ˜¯/å¦å†³ç­–çš„æƒ…å†µã€‚
* ğŸ”¥å®ç°æ³¨æ„äº‹é¡¹
	åœ¨å®ç°Actor-Criticæ¨¡å‹æ—¶ï¼Œé€‰æ‹©å“ªç§åˆ†å¸ƒç±»å‹å–å†³äºç¯å¢ƒçš„åŠ¨ä½œç©ºé—´ç±»å‹ï¼ˆè¿ç»­è¿˜æ˜¯ç¦»æ•£ï¼‰å’ŒåŠ¨ä½œç»´åº¦ä¹‹é—´æ˜¯å¦ç‹¬ç«‹ã€‚å¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œ**æ­£æ€åˆ†å¸ƒ**å’Œ**å¯¹è§’é«˜æ–¯åˆ†å¸ƒ**æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹å¼æ¥æ¨¡æ‹ŸåŠ¨ä½œçš„ä¸ç¡®å®šæ€§å’Œå¤šæ ·æ€§ã€‚å¯¹äºç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œ**åˆ†ç±»åˆ†å¸ƒ**æä¾›äº†ä¸€ç§ç®€æ´çš„æ–¹å¼æ¥è¡¨ç¤ºæ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼Œå¹¶ä»ä¸­é‡‡æ ·ã€‚
	
	åœ¨å®šä¹‰åˆ†å¸ƒå’Œé‡‡æ ·åŠ¨ä½œæ—¶ï¼Œé‡è¦çš„æ˜¯è¦ç¡®ä¿åŠ¨ä½œçš„é‡‡æ ·è¿‡ç¨‹æ˜¯å¯å¾®çš„ï¼Œä»¥ä¾¿å¯ä»¥é€šè¿‡åå‘ä¼ æ’­ç®—æ³•æ¥ä¼˜åŒ–ç­–ç•¥ç½‘ç»œçš„å‚æ•°ã€‚PyTorchçš„åˆ†å¸ƒåº“å·²ç»è€ƒè™‘åˆ°äº†è¿™ä¸€ç‚¹ï¼Œä½¿å¾—åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å®ç°Actor-Criticæ¨¡å‹å˜å¾—ç›¸å¯¹ç®€å•ä¸”é«˜æ•ˆã€‚

ğŸŒ•Entropy

åœ¨PPOï¼ˆProximal Policy Optimizationï¼‰çš„è®ºæ–‡ä¸­ï¼Œè™½ç„¶æ²¡æœ‰ç›´æ¥æåˆ°ä½¿ç”¨äº¤å‰ç†µï¼ˆcross-entropyï¼‰ä½œä¸ºæŸå¤±å‡½æ•°çš„éƒ¨åˆ†ï¼Œä½†åœ¨å®ç°PPOç®—æ³•çš„ä»£ç ä¸­ï¼Œäº¤å‰ç†µå¸¸è¢«ç”¨äºå¤„ç†ç‰¹å®šçš„ä¼˜åŒ–é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä¸ç­–ç•¥ç½‘ç»œçš„è¾“å‡ºç›¸å…³çš„é—®é¢˜ã€‚è¿™ç§çœ‹ä¼¼çš„å·®å¼‚å®é™…ä¸Šæºäºè®ºæ–‡æè¿°ç®—æ³•çš„ç†è®ºæ¡†æ¶ä¸ç®—æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å…·ä½“å®ç°ä¹‹é—´çš„åŒºåˆ«ã€‚ä¸‹é¢æˆ‘ä»¬æ¥æ¢è®¨ä¸ºä»€ä¹ˆä¼šæœ‰è¿™ç§å·®å¼‚ã€‚
* ğŸŒ›PPOè®ºæ–‡çš„é‡ç‚¹
	PPOè®ºæ–‡ä¸»è¦å…³æ³¨çš„æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥æé«˜å¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„ç›®æ ‡å‡½æ•°ï¼Œè¯¥å‡½æ•°é™åˆ¶äº†æ–°æ—§ç­–ç•¥ä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œé¿å…äº†åœ¨ç­–ç•¥æ›´æ–°æ—¶å‡ºç°æ€§èƒ½å‰§çƒˆä¸‹é™çš„é—®é¢˜ã€‚è¿™ç§æ–¹æ³•ä¸»è¦é€šè¿‡å‰ªè£æ¦‚ç‡æ¯”ç‡æˆ–ä½¿ç”¨KLæ•£åº¦æ¥å®ç°ï¼Œè€Œæ²¡æœ‰ç›´æ¥æåˆ°äº¤å‰ç†µã€‚
* ğŸŒ›äº¤å‰ç†µåœ¨PPOå®ç°ä¸­çš„ä½œç”¨
	1. **ç­–ç•¥ç½‘ç»œçš„è¾“å‡º**ï¼šåœ¨å®ç°PPOæ—¶ï¼Œç­–ç•¥ç½‘ç»œé€šå¸¸è¾“å‡ºåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒã€‚ä¸ºäº†è®­ç»ƒè¿™ä¸ªç½‘ç»œä½¿å…¶è¾“å‡ºä¸æœŸæœ›çš„åŠ¨ä½œåˆ†å¸ƒå°½å¯èƒ½æ¥è¿‘ï¼Œäº¤å‰ç†µæ˜¯ä¸€ä¸ªéå¸¸è‡ªç„¶çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒæ˜¯è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„å¸¸ç”¨æ–¹æ³•ã€‚
	2. **é¼“åŠ±æ¢ç´¢**ï¼šä½¿ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±çš„ä¸€éƒ¨åˆ†å¯ä»¥å¸®åŠ©ç®—æ³•åœ¨æ¢ç´¢ç¯å¢ƒæ—¶ä¿æŒä¸€å®šçš„éšæœºæ€§ï¼Œè¿™å¯¹äºå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„æ¢ç´¢éå¸¸é‡è¦ã€‚
	3. **ç¨³å®šæ€§å’Œæ€§èƒ½**ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼Œç ”ç©¶è€…å‘ç°æ·»åŠ äº¤å‰ç†µæŸå¤±å¯ä»¥å¸®åŠ©æé«˜ç®—æ³•çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è™½ç„¶è¿™å¯èƒ½ä¸æ˜¯è®ºæ–‡ä¸­åŸå§‹ç®—æ³•æè¿°çš„ä¸€éƒ¨åˆ†ï¼Œä½†åœ¨å®é™…å®ç°å’Œè°ƒä¼˜è¿‡ç¨‹ä¸­ï¼Œæ ¹æ®å…·ä½“ä»»åŠ¡çš„éœ€è¦å¯¹ç®—æ³•è¿›è¡Œé€‚å½“çš„ä¿®æ”¹æ˜¯å¾ˆå¸¸è§çš„ã€‚
* ğŸŒ›ç»“è®º
	PPOè®ºæ–‡ä¸­æ²¡æœ‰æ˜ç¡®æåˆ°äº¤å‰ç†µï¼Œæ˜¯å› ä¸ºè®ºæ–‡èšç„¦äºæè¿°ç®—æ³•çš„æ ¸å¿ƒç†è®ºå’Œæ–¹æ³•ï¼Œè€Œåœ¨å…·ä½“å®ç°å’Œä¼˜åŒ–ç®—æ³•æ€§èƒ½æ—¶ï¼Œæ ¹æ®å®é™…éœ€è¦å¼•å…¥**äº¤å‰ç†µ**ä½œä¸ºæŸå¤±å‡½æ•°çš„ä¸€éƒ¨åˆ†**æ˜¯å®è·µä¸­çš„å¸¸è§åšæ³•**ã€‚è¿™ç§å®è·µåæ˜ äº†ç†è®ºä¸å®é™…åº”ç”¨ä¹‹é—´çš„æ¡¥æ¢ï¼Œå±•ç¤ºäº†åœ¨å°†ç†è®ºåº”ç”¨åˆ°å®é™…é—®é¢˜ä¸­æ—¶å¯èƒ½éœ€è¦çš„è°ƒæ•´å’Œä¼˜åŒ–ã€‚
